{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89747490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T21:51:58.087670Z",
     "start_time": "2022-08-16T21:51:57.980614Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Decision Tree functions\n",
    "def decision_tree_train(X_train, y_train, selected_features, target, d = 10, m=1, print_results = True):\n",
    "  \n",
    "    clf = DecisionTreeClassifier(max_depth=d, min_samples_leaf = m, random_state=123)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    class_report = classification_report(y_train, y_pred,output_dict=True)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "        \n",
    "    fp_rate = fp/(fp+tn)\n",
    "    fn_rate = fn/(fn+tp)\n",
    "    tp_rate = tp/(tp+fn)\n",
    "    tn_rate = tn/(fp+tn)\n",
    "    if print_results:\n",
    "        print(f\"TRAINING RESULTS: {type(clf).__name__}\")\n",
    "        print(f\"Using features: {selected_features}\")\n",
    "        print(f\"Depth of {clf.max_depth}\")\n",
    "        print(f\"Min Sample Leaf of {clf.min_samples_leaf}\")\n",
    "        print(\"----------------\")\n",
    "        print(f\"Accuracy score on training set is: {accuracy:.2f}\")\n",
    "        print(classification_report(y_train, y_pred))\n",
    "\n",
    "\n",
    "        print(f\"False positive rate: {fp/(fp+tn):.2%}\")\n",
    "        print(f\"False negative rate: {fn/(fn+tp):.2%}\")\n",
    "        print(f\"True positive rate: {tp/(tp+fn):.2%}\")\n",
    "        print(f\"True negative rate: {tn/(fp+tn):.2%}\")\n",
    "        print(\"----------------\")\n",
    "    \n",
    "    train_report = {'d':clf.max_depth, \n",
    "                    'm':clf.min_samples_leaf,\n",
    "                    'accuracy':accuracy, \n",
    "                    'precision':class_report['1']['precision'], \n",
    "                    'recall':class_report['1']['recall'],\n",
    "                   'fp_rate':fp_rate,\n",
    "                   'fn_rate':fn_rate,\n",
    "                   'tp_rate':tp_rate,\n",
    "                   'tn_rate':tn_rate}\n",
    "    \n",
    "    return clf, train_report\n",
    "\n",
    "def classifier_validate(X_validate, y_validate, clf, print_results=True):\n",
    "    d = clf.max_depth\n",
    "    accuracy = clf.score(X_validate, y_validate)\n",
    "\n",
    "\n",
    "    # Produce y_predictions that come from the X_validate\n",
    "    y_pred = clf.predict(X_validate)\n",
    "    \n",
    "    class_report = classification_report(y_validate, y_pred,output_dict=True)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_validate, y_pred).ravel()\n",
    "    \n",
    "    fp_rate = fp/(fp+tn)\n",
    "    fn_rate = fn/(fn+tp)\n",
    "    tp_rate = tp/(tp+fn)\n",
    "    tn_rate = tn/(fp+tn)\n",
    "    # Compare actual y values (from validate) to predicted y_values from the model run on X_validate\n",
    "    if print_results:\n",
    "        print(f\"-----VALIDATE RESULTS: {type(clf).__name__}-----\")\n",
    "        print(f\"Using features: {selected_features}\")\n",
    "        print(f\"Depth of {clf.max_depth}\")\n",
    "        print(f\"Min Sample Leaf of {clf.min_samples_leaf}\")\n",
    "        print(classification_report(y_validate, y_pred))\n",
    "\n",
    "        print(f'Accuracy on validate set: {accuracy:.2f}')\n",
    "    validate_report = {'d':clf.max_depth, \n",
    "                       'm':clf.min_samples_leaf,\n",
    "                    'accuracy':accuracy, \n",
    "                    'precision':class_report['1']['precision'], \n",
    "                    'recall':class_report['1']['recall'],\n",
    "                   'fp_rate':fp_rate,\n",
    "                   'fn_rate':fn_rate,\n",
    "                   'tp_rate':tp_rate,\n",
    "                   'tn_rate':tn_rate}\n",
    "    \n",
    "    return validate_report\n",
    "\n",
    "# Random Forest Functions\n",
    "def random_forest_train(X_train, y_train, selected_features, target, d = 10, m=1, print_results = True):\n",
    "  \n",
    "    clf = RandomForestClassifier(max_depth=d, min_samples_leaf = m, random_state=123)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    class_report = classification_report(y_train, y_pred,output_dict=True)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "        \n",
    "    fp_rate = fp/(fp+tn)\n",
    "    fn_rate = fn/(fn+tp)\n",
    "    tp_rate = tp/(tp+fn)\n",
    "    tn_rate = tn/(fp+tn)\n",
    "    if print_results:\n",
    "        print(f\"TRAINING RESULTS: {type(clf).__name__}\")\n",
    "        print(f\"Using features: {selected_features}\")\n",
    "        print(f\"Depth of {clf.max_depth}\")\n",
    "        print(f\"Min Sample Leaf of {clf.min_samples_leaf}\")\n",
    "        print(\"----------------\")\n",
    "        print(classification_report(y_train, y_pred))\n",
    "\n",
    "\n",
    "        print(f\"False positive rate: {fp/(fp+tn):.2%}\")\n",
    "        print(f\"False negative rate: {fn/(fn+tp):.2%}\")\n",
    "        print(f\"True positive rate: {tp/(tp+fn):.2%}\")\n",
    "        print(f\"True negative rate: {tn/(fp+tn):.2%}\")\n",
    "        print(\"----------------\")\n",
    "    \n",
    "    train_report = {'d':clf.max_depth, \n",
    "                    'm':clf.min_samples_leaf,\n",
    "                    'accuracy':accuracy, \n",
    "                    'precision':class_report['1']['precision'], \n",
    "                    'recall':class_report['1']['recall'],\n",
    "                   'fp_rate':fp_rate,\n",
    "                   'fn_rate':fn_rate,\n",
    "                   'tp_rate':tp_rate,\n",
    "                   'tn_rate':tn_rate}\n",
    "    \n",
    "    return clf, train_report\n",
    "\n",
    "# KNN Functions\n",
    "def knn_train(X_train, y_train, selected_features, target, k=1, print_results = True):\n",
    "  \n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    class_report = classification_report(y_train, y_pred,output_dict=True)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "        \n",
    "    fp_rate = fp/(fp+tn)\n",
    "    fn_rate = fn/(fn+tp)\n",
    "    tp_rate = tp/(tp+fn)\n",
    "    tn_rate = tn/(fp+tn)\n",
    "    if print_results:\n",
    "        print(f\"TRAINING RESULTS: {type(clf).__name__}\")\n",
    "        print(f\"Using features: {selected_features}\")\n",
    "        print(f\"K of {clf.n_neighbors}\")\n",
    "        print(\"----------------\")\n",
    "        print(classification_report(y_train, y_pred))\n",
    "\n",
    "\n",
    "        print(f\"False positive rate: {fp/(fp+tn):.2%}\")\n",
    "        print(f\"False negative rate: {fn/(fn+tp):.2%}\")\n",
    "        print(f\"True positive rate: {tp/(tp+fn):.2%}\")\n",
    "        print(f\"True negative rate: {tn/(fp+tn):.2%}\")\n",
    "        print(\"----------------\")\n",
    "    \n",
    "    train_report = {'k':clf.n_neighbors, \n",
    "                    'accuracy':accuracy, \n",
    "                    'precision':class_report['1']['precision'], \n",
    "                    'recall':class_report['1']['recall'],\n",
    "                   'fp_rate':fp_rate,\n",
    "                   'fn_rate':fn_rate,\n",
    "                   'tp_rate':tp_rate,\n",
    "                   'tn_rate':tn_rate}\n",
    "    \n",
    "    return clf, train_report\n",
    "\n",
    "def knn_validate(X_validate, y_validate, clf, print_results=True):\n",
    "    accuracy = clf.score(X_validate, y_validate)\n",
    "\n",
    "\n",
    "    # Produce y_predictions that come from the X_validate\n",
    "    y_pred = clf.predict(X_validate)\n",
    "    \n",
    "    class_report = classification_report(y_validate, y_pred,output_dict=True)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_validate, y_pred).ravel()\n",
    "    \n",
    "    fp_rate = fp/(fp+tn)\n",
    "    fn_rate = fn/(fn+tp)\n",
    "    tp_rate = tp/(tp+fn)\n",
    "    tn_rate = tn/(fp+tn)\n",
    "    # Compare actual y values (from validate) to predicted y_values from the model run on X_validate\n",
    "    if print_results:\n",
    "        print(f\"-----VALIDATE RESULTS: {type(clf).__name__}-----\")\n",
    "        print(f\"Using features: {selected_features}\")\n",
    "        print(f\"K of {clf.n_neighbors}\")\n",
    "        print(classification_report(y_validate, y_pred))\n",
    "\n",
    "        print(f'Accuracy on validate set: {accuracy:.2f}')\n",
    "    validate_report = {'k':clf.n_neighbors, \n",
    "                    'accuracy':accuracy, \n",
    "                    'precision':class_report['1']['precision'], \n",
    "                    'recall':class_report['1']['recall'],\n",
    "                   'fp_rate':fp_rate,\n",
    "                   'fn_rate':fn_rate,\n",
    "                   'tp_rate':tp_rate,\n",
    "                   'tn_rate':tn_rate}\n",
    "    \n",
    "    return validate_report\n",
    "\n",
    "# Logistical Regression Functions\n",
    "def logistic_regression_train(X_train, y_train, selected_features, target, c=1, print_results = True):\n",
    "  \n",
    "    clf = LogisticRegression(C=c)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    class_report = classification_report(y_train, y_pred,output_dict=True)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "        \n",
    "    fp_rate = fp/(fp+tn)\n",
    "    fn_rate = fn/(fn+tp)\n",
    "    tp_rate = tp/(tp+fn)\n",
    "    tn_rate = tn/(fp+tn)\n",
    "    if print_results:\n",
    "        print(f\"TRAINING RESULTS: {type(clf).__name__}\")\n",
    "        print(f\"Using features: {selected_features}\")\n",
    "        print(f\"C of {clf.C}\")\n",
    "        print(\"----------------\")\n",
    "        print(classification_report(y_train, y_pred))\n",
    "\n",
    "\n",
    "        print(f\"False positive rate: {fp/(fp+tn):.2%}\")\n",
    "        print(f\"False negative rate: {fn/(fn+tp):.2%}\")\n",
    "        print(f\"True positive rate: {tp/(tp+fn):.2%}\")\n",
    "        print(f\"True negative rate: {tn/(fp+tn):.2%}\")\n",
    "        print(\"----------------\")\n",
    "    \n",
    "    train_report = {'c':clf.C, \n",
    "                    'accuracy':accuracy, \n",
    "                    'precision':class_report['1']['precision'], \n",
    "                    'recall':class_report['1']['recall'],\n",
    "                   'fp_rate':fp_rate,\n",
    "                   'fn_rate':fn_rate,\n",
    "                   'tp_rate':tp_rate,\n",
    "                   'tn_rate':tn_rate}\n",
    "    \n",
    "    return clf, train_report\n",
    "\n",
    "def logisitic_regression_validate(X_validate, y_validate, clf, print_results=True):\n",
    "    accuracy = clf.score(X_validate, y_validate)\n",
    "\n",
    "    # Produce y_predictions that come from the X_validate\n",
    "    y_pred = clf.predict(X_validate)\n",
    "    \n",
    "    class_report = classification_report(y_validate, y_pred,output_dict=True)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_validate, y_pred).ravel()\n",
    "    \n",
    "    fp_rate = fp/(fp+tn)\n",
    "    fn_rate = fn/(fn+tp)\n",
    "    tp_rate = tp/(tp+fn)\n",
    "    tn_rate = tn/(fp+tn)\n",
    "    # Compare actual y values (from validate) to predicted y_values from the model run on X_validate\n",
    "    if print_results:\n",
    "        print(f\"-----VALIDATE RESULTS: {type(clf).__name__}-----\")\n",
    "        print(f\"Using features: {selected_features}\")\n",
    "        print(f\"C of {clf.C}\")\n",
    "        print(classification_report(y_validate, y_pred))\n",
    "\n",
    "        print(f'Accuracy on validate set: {accuracy:.2f}')\n",
    "    validate_report = {'c':clf.C, \n",
    "                    'accuracy':accuracy, \n",
    "                    'precision':class_report['1']['precision'], \n",
    "                    'recall':class_report['1']['recall'],\n",
    "                   'fp_rate':fp_rate,\n",
    "                   'fn_rate':fn_rate,\n",
    "                   'tp_rate':tp_rate,\n",
    "                   'tn_rate':tn_rate}\n",
    "    \n",
    "    return validate_report\n",
    "\n",
    "def consolidate_results(train_results_df, validate_results_df, join_on):\n",
    "    \"\"\" Consolidates the results of fitting the models on the train dataset and testing on the validate (or test) set. Takes as arguments the classification report and relevant metrics for both the train and validate (test) sets, as well as the parameter(s) to merge the two results on (usually the hyperparameters tested) in the form of a list. Calculates the difference in performance between train and validate and outputs a dataframe of the combined results to allow for easy plotting with Seaborn.\"\"\"\n",
    "    \n",
    "    combined_df = train_results_df.merge(validate_results_df,on=join_on, suffixes=['_train','_validate'])\n",
    "    # Calculate difference in accuracy between train and validate sets\n",
    "    combined_df[\"accuracy_diff\"] = combined_df.accuracy_validate-combined_df.accuracy_train\n",
    "    combined_df[\"precision_diff\"] = combined_df.precision_validate-combined_df.precision_train\n",
    "    combined_df[\"recall_diff\"] = combined_df.recall_validate-combined_df.recall_train\n",
    "    combined_df[\"accuracy_pct_diff\"] = (combined_df.accuracy_validate-combined_df.accuracy_train)/combined_df.accuracy_train\n",
    "\n",
    "    return combined_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
